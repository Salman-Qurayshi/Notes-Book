# Docker

> Created by Salman Al Qureshi
Linkedin: Salman Qureshi
> 
> 
> http://www.linkedin.com/in/salman-qureshi-4aa41a247
> 

## **Docker Basics**

### **Docker Overview**

**Docker** is a platform (tool) that helps you build, run, and manage applications by using **containers** (small, self-contained environments).

**Docker** is a platform that simplifies the process of building, shipping, and running applications. It uses a technology called **containerization**.

Think of a **container** as a lightweight, standalone, executable package of software that includes everything the application needs to run: code, runtime, system tools, system libraries, settings.

- **Key Concepts:**
    - **Containers**:
        - Containers are isolated environments (small, separate areas) where your application runs. They're lightweight (small and fast) because they share the host system's OS kernel (core part of the operating system).
    - **Images**:
        - An image is like a blueprint (a detailed plan) for creating a container. It includes everything the application needs to run. Once built, you can use the same image to create multiple containers.
    - **Docker Engine**:
        - The Docker Engine is the core software (main program) that runs and manages containers.
    - **Dockerfile**:
        - A Dockerfile is a script (a set of instructions) that tells Docker how to build an image. It contains commands like `FROM` (start from a specific image) and `RUN` (execute a command).
    - **Docker Hub**:
        - Docker Hub is an online service (cloud-based repository) where you can share, store, and download container images.
    - **Volumes**:
        - Volumes are storage locations (places to save data) used by containers. They help keep data safe even if the container is deleted.
    - **Networking**:
        - Docker networking allows containers to talk to each other and connect to external networks. It uses different network drivers (tools) like `bridge` (default, private network) and `host` (directly uses the host's network).
- **Basic Docker Commands:**
    - **`docker run`**:
        - This command creates and starts a container from an image.
        - **Example**: `docker run -it ubuntu` (Starts an interactive Ubuntu container) 
        # -t : Stand for tag the image --> Give name of the image
    - **`docker ps`**:
        - Shows a list of running containers.
        - **Example**: `docker ps -a` (Shows all containers, even those that are stopped)
    - **`docker build`**:
        - Creates an image from a Dockerfile.
        - **Example**: `docker build -t myapp .` (Builds an image named `myapp` from the current directory)
    - **`docker pull`**:
        - Downloads an image from Docker Hub.
        - **Example**: `docker pull nginx` (Downloads the Nginx image)
    - **`docker push`**:
        - Uploads an image to a Docker registry (storage service).
        - **Example**: `docker push myrepo/myapp` (Uploads the image to your repository)
    - **`docker exec`**:
        - Runs a command inside a running container.
        - **Example**: `docker exec -it mycontainer bash` (Opens a terminal inside `mycontainer`)
    - **`docker stop`**:
        - Stops a running container.
        - **Example**: `docker stop mycontainer` (Stops the `mycontainer`)

### **Docker architecture**

- **Docker architecture** 
it consists of the Docker Client (interface), Docker Daemon (core service), Docker Engine (platform), Docker Images (blueprints), Docker Containers (running instances), Docker Registries (storage), Docker Networks (communication), and Docker Volumes (storage for data). Together, these components allow you to build, run, and manage containers efficiently.

> Points to ponder:
We cannot Edit docker images it’s always read only, recreate can happen.

Docker Is a Platform as a service (PAAS) That uses Os level virtualization whereas Virtual Machines (VMs) Uses Hardware level virtualization.

Benifits of Docker is:
Standard —> portable
Lightweight —> by using images like alpine(uses apk pckage manager) and busybox 
Secure —> it’s already isolated 

server & client:
client is the cli we interact with and the server is just the daemon running in the bg
so if we stop the server that is the daemon then we won’t be able to interact with the server so we will be able to run commands like docker version etc but not docker images ( the commands that needs the server )  

Note: Docker server can also be on another machine and not on the client
> 

### Docker Run Command

- 
    
    The `docker run` command is used to **create and start** a new container.
    
    **Key Functions:
    Pulls the image**: If the specified image isn't available locally, Docker will **pull it from a registry** (like Docker Hub) before creating the container.
    
    **Creates a container**: Based on the pulled image, Docker creates a **new container instance**.
    
    **Runs the container**: The container starts executing the **specified command** or the **default command** defined in the Dockerfile.
    
    **Command Structure:**
    
    - **`docker run [OPTIONS] IMAGE [COMMAND] [ARG...]`**
        - **`IMAGE`**: The name of the Docker image to use.
        - **`COMMAND`**: The command to run inside the container (optional).
        - **`ARG...`**: Arguments for the command (optional).
    
    **Example:**
    
    - **`docker run hello-world`**
        - This command pulls the **`hello-world`** image, creates a container from it, and runs the **default command** within the image, which is to print **"Hello from Docker!"**
    
    **Common Options:**
    
    - **`d`** or **`-detach`**: Runs the container in **detached mode**, allowing it to run in the background.
    - **`i`**: Allocates a **pseudo-TTY** for the container.
    - **`t`**: Allocates a **pseudo-TTY** and keeps **stdin open**.
    - **`p`**: Maps a **port from the container** to the host. # Example: -p 8080:80 . here the left side port is for the host system while the right side is for the docker container
    - **`v`**: Mounts a **volume from the host** to the container.
    - `sh -c`: this is used to run script.
    - `--restart=always`: This is used to restart the container automatically if it stop e.g host restart or any other issue occurs
    

### **Docker Start Command**

- 
    
    The `docker start` command is used to **start one or more stopped containers**. Unlike the `docker run` command, which creates and starts a new container, the `docker start` command **only starts** containers that have already been created and stopped.
    
    **Key Functions:**
    
    - **Restarts a container**: If you have previously stopped a container, you can use `docker start` to start it again without creating a new instance.
    - **Keeps the existing state**: The container starts with the same state it had when it was stopped, including its filesystem, network settings, and any processes that were running.
    
    **Command Structure:**
    
    - **`docker start [OPTIONS] CONTAINER [CONTAINER...]`**
        - **`CONTAINER`**: The name or ID of the container(s) you want to start. You can specify multiple containers by separating their names or IDs with a space.
    
    **Common Options:**
    
    - **`a`** or **`-attach`**: Attach to the container's STDOUT, similar to running it in the foreground. This option is useful if you want to see the output of the container as it runs.
    - **`i`** or **`-interactive`**: Attach to the container's STDIN, allowing you to interact with it. This option is useful for containers that require user input.
    
    **Example:**
    
    - **`docker start my_container`**
        - This command starts a container named **`my_container`** that was previously stopped.
    - **`docker start -a -i my_container`**
        - This command starts **`my_container`**, attaches to it, and allows you to interact with it.
    

### Docker Tag Command

- 
    
    The `docker tag` command is used to create a new tag for an existing Docker image. A tag is essentially an alias attached to an image, allowing you to reference the image by that alias rather than the image ID. Tags are commonly used to version images, organize them, and make them easier to reference.
    
    ### Key Functions:
    
    - **Create a new tag**: The `docker tag` command allows you to apply a new tag to an existing image, which can be useful for versioning or categorizing your images.
    - **Push to a registry**: Once an image is tagged, it can be pushed to a Docker registry (like Docker Hub) under the new tag, making it easier to manage and distribute.
    
    ### Command Structure:
    
    `docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]`
    
    - **SOURCE_IMAGE[]**: The name (and optional tag) of the image you want to tag. If no tag is provided, Docker defaults to the `latest` tag.
    - **TARGET_IMAGE[]**: The new tag you want to create. This can include a new image name and/or a new tag. If no tag is specified, Docker will also default to `latest`.
    
    ### Example:
    
    `docker tag myapp:1.0 myapp:1.1`
    
    - This command tags the image `myapp:1.0` with a new tag `myapp:1.1`. Now, the same image can be referenced by either `myapp:1.0` or `myapp:1.1`.
    
    Another example for pushing to a registry:
    
    `docker tag myapp:1.0 username/myapp:1.0`
    
    - This command tags the `myapp:1.0` image with a new tag `username/myapp:1.0`, which is then ready to be pushed to Docker Hub under the user's repository.
    

### Docker Push Command

- 
    
    The `docker push` command is used to upload or "push" a Docker image from your local machine to a Docker registry, such as Docker Hub. This allows you to share images with others or use them across different environments.
    
    ### Key Functions:
    
    - **Uploads the image**: The primary function of `docker push` is to upload your tagged Docker image to a registry so that it can be accessed and pulled by others.
    - **Version control**: By pushing images with different tags, you can maintain versions of your application in the registry.
    
    ### Command Structure:
    
    `docker push [OPTIONS] NAME[:TAG]`
    
    - **NAME[]**: The name of the image you want to push, including the tag. The name usually follows the format `username/repository:tag`.
    
    ### Example:
    
    1. **Tag the Image** (if not already tagged):
        
        Before pushing, ensure your image is tagged correctly. For example, if your image is named `myapp` and you want to push it to your Docker Hub repository under the `username` account:
        
        `docker tag myapp:latest username/myapp:latest`
        
        Here, `username` is your Docker Hub username, and `myapp` is the repository name.
        
    2. **Log in to Docker Hub**:
        
        You need to be logged in to your Docker Hub account to push images:
        
        `docker login`
        
        Follow the prompts to enter your Docker Hub username and password.
        
    3. **Push the Image**:
        
        Once tagged and logged in, push the image to Docker Hub:
        
        `docker push username/myapp:latest`
        
        This command uploads the `latest` tagged image in the `myapp` repository under your `username` account.
        
    

### Docker Commit Command

- 
    
    The `docker commit` command is used to create a new image from an existing container's changes. This command is especially useful when you've made modifications to a running container (such as installing new software or changing configurations) and want to save those changes as a new image.
    
    ### Key Functions:
    
    - **Captures the Container State**: The primary function of `docker commit` is to take a snapshot of the current state of a container, including any changes made to its filesystem.
    - **Creates a New Image**: Based on the snapshot, Docker creates a new image that you can then run, share, or push to a registry.
    
    ### Command Structure:
    
    `docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]`
    
    - **CONTAINER**: The ID or name of the container you want to commit.
    - **REPOSITORY[]**: The repository and tag name you want to assign to the new image (optional).
    
    ### Example:
    
    1. **Run a Container**:
        
        Start by running a container, for example, from the `ubuntu` image:
        
        `docker run -it ubuntu`
        
        This opens a shell where you can make changes, such as installing software.
        
    2. **Make Changes**:
        
        Inside the running container, you might install a package or modify a configuration file. For instance:
        
        `apt-get update && apt-get install -y curl`
        
    3. **Commit the Changes**:
        
        Once you're satisfied with the changes, exit the container, then commit those changes to a new image:
        
        `docker commit <container_id> username/mynewimage:version1`
        
        Replace `<container_id>` with your container's ID or name. This command creates a new image named `mynewimage` with the tag `version1` under your Docker Hub username.
        
    4. **Use the New Image**:
        
        You can now run a container using the new image:
        
        `docker run -it username/mynewimage:version1`
        
    

### Docker Build Command

- 
    
    The `docker build` command is used to create a Docker image from a set of instructions defined in a Dockerfile. This is a key part of Docker's process, allowing you to automate the creation of images with a consistent and repeatable process.
    
    ### Key Functions:
    
    - **Reads Instructions**: Docker reads the instructions in a Dockerfile, which typically include steps like specifying a base image, copying files, installing packages, and setting environment variables.
    - **Creates an Image**: Based on these instructions, Docker creates a new image that can be reused, shared, or pushed to a registry.
    
    ### Command Structure:
    
    `docker build [OPTIONS] PATH | URL | -`
    
    - **PATH | URL | -**: This specifies the location of the Dockerfile. It can be a local directory path, a URL to a git repository, or a  for reading from stdin.
    
    ### Example:
    
    1. **Create a Dockerfile**:
        
        In your project directory, create a `Dockerfile` with instructions, for example:
        
        ```bash
        bashCopy code
        # Use the official Ubuntu base image
        FROM ubuntu:latest
        
        # Install curl
        RUN apt-get update && apt-get install -y curl
        
        # Set a working directory
        WORKDIR /app
        
        # Copy files from the current directory to /app in the container
        COPY . /app
        
        # Set the command to run when the container starts
        CMD ["bash"]
        
        ```
        
    2. **Build the Image**:
        
        Run the following command in the directory containing your Dockerfile:
        
        `docker build -t myimage:1.0 .`
        
        - **t myimage:1.0**: Tags the image with the name `myimage` and version `1.0`.
        - **.**: The dot specifies the build context, meaning Docker should use the files in the current directory.
    3. **View the Image**:
        
        After the build process completes, you can view your new image with:
        
        `docker images`
        
    4. **Run a Container from the Image**:
        
        You can now run a container using your newly created image:
        
        `docker run -it myimage:1.0`
        
    
    ### Common Options:
    
    - **t, --tag**: Name and optionally tag the image in the `name:tag` format. < `docker build -t myimage:latest .` >
    - **f, --file**: Specify a Dockerfile to use, instead of the default one named `Dockerfile`. < `docker build -f Dockerfile.dev .` >
    - **-build-arg**: Set build-time variables that can be accessed in the Dockerfile. < `docker build --build-arg USER=example .` >
    - **-no-cache**: Build the image without using cache, forcing a fresh build. < `docker build --no-cache -t myimage:1.0 .` >
    

### **Dockerfiles: Building and Using Docker Images**

- 
    
    A Dockerfile is a text document that contains instructions for Docker to build an image. Each instruction in a Dockerfile creates a layer in the image, making it possible to reuse and cache layers, which speeds up the build process.
    
    ### Key Instructions in a Dockerfile:
    
    - **`FROM`**: Specifies the base image to use for the new image. This is usually the first instruction in a Dockerfile.
        
        Example: `FROM ubuntu:latest`
        
    - **`RUN`**: Executes commands in a new layer on top of the current image and commits the results. It's used to install software packages or run other setup commands.
        
        Example: `RUN apt-get update && apt-get install -y nginx`
        
    - **`MAINTAINER`**: Used to specify the author or maintainer of the Docker image. This instruction is deprecated in favor of using labels.
        
        Example: `MAINTAINER John Doe <john.doe@example.com>`
        
    - **`LABEL`**: Adds metadata to the image, such as information about the image version, author, or other details. It's the modern alternative to `MAINTAINER`.
        
        Example: `LABEL maintainer="John Doe <john.doe@example.com>" version="1.0"`
        
    - **`COPY`**: Copies files or directories from the host filesystem into the container. It’s commonly used to copy application code or configuration files.
        
        Example: `COPY . /app`
        
    - **`ADD`**: Similar to `COPY`, but with some additional features. It can copy files from a URL or automatically extract tar files.
        
        Example: `ADD https://example.com/somefile.tar.gz /app/`
        
    - **`WORKDIR`**: Sets the working directory for any subsequent instructions in the Dockerfile. It’s like `cd` in a shell.
        
        Example: `WORKDIR /app`
        
    - **`CMD`**: Specifies the default command to run when a container is started. If a user specifies a command when running the container, the `CMD` instruction will be overridden.
        
        Example: `CMD ["nginx", "-g", "daemon off;"]`
        
    - **`ENTRYPOINT`**: Similar to `CMD`, but it’s not overridden by arguments passed to the `docker run` command. It allows you to configure a container that will run as an executable.
        
        Example: `ENTRYPOINT ["nginx", "-g", "daemon off;"]`
        
    - **`ENV`**: Sets environment variables in the container, which can be used during the build process or when the container is running.
        
        Example: `ENV NODE_ENV production`
        
    - **`EXPOSE`**: Informs Docker that the container will listen on the specified network ports at runtime. It doesn’t actually publish the ports; that’s done with the `p` flag in `docker run`.
        
        Example: `EXPOSE 80`
        
    - **`VOLUME`**: Creates a mount point with a specified path and marks it as holding externally mounted volumes from the host or other containers.
        
        Example: `VOLUME /data`
        
    - **`USER`**: Specifies the user to use when running the image and for any `RUN`, `CMD`, and `ENTRYPOINT` instructions that follow it in the Dockerfile.
        
        Example: `USER nginx`
        
    - **`ARG`**: Defines build-time variables that users can pass at build time to the builder with the `-build-arg` option.
        
        Example: `ARG build_version=1.0`
        
    - **`ONBUILD`**: Adds a trigger instruction to the image that will be executed when the image is used as a base for another build.
        
        Example: `ONBUILD COPY . /app`
        
    
    ### Example Dockerfile
    
    Here’s an example Dockerfile that creates a basic Node.js application:
    
    ```bash
    # Use the official Node.js image as the base image
    FROM node:14
    
    # Set the working directory inside the container
    WORKDIR /app
    
    # Copy the package.json and package-lock.json files to the working directory
    COPY package*.json ./
    
    # Install dependencies
    RUN npm install
    
    # Copy the rest of the application code to the working directory
    COPY . .
    
    # Expose the application’s port
    EXPOSE 8080
    
    # Define an environment variable
    ENV NODE_ENV production
    
    # Specify the command to run the application
    CMD ["node", "app.js"]
    
    ```
    
    ### Building and Using Docker Images
    
    1. **Create a Dockerfile**: Save the above text into a file named `Dockerfile` in your project directory.
    2. **Build the Image**:
        - Run `docker build -t my-node-app .` in your terminal, where `my-node-app` is the name you want to give your image, and the `.` specifies the current directory as the build context.
    3. **Run a Container**:
        - Once the image is built, you can run a container using `docker run -p 8080:8080 my-node-app`.
        - This command will start the application and map port 8080 of the container to port 8080 on your host machine.
    4. **Push the Image to Docker Hub** (optional):
        - If you want to share your image, you can push it to Docker Hub. First, tag your image with your Docker Hub username: `docker tag my-node-app yourusername/my-node-app`.
        - Then push the image: `docker push yourusername/my-node-app`.
    
    > Difference between CMD and ENTRYPOINT is that ENTRYPOINT cannot be overwritten while CMD can be
    > 
    

### Understanding `-rm` in Docker Build
AND 
Intermediate Images

- 
    
    
    When Docker builds an image, it creates temporary containers based on each `FROM` instruction in your Dockerfile. These containers are used to execute the commands specified in the `RUN`, `COPY`, and `ADD` instructions. These temporary containers are called intermediate images.
    
    **The Role of** `-rm` 
    By default, Docker removes these intermediate containers after a successful build. This is to clean up and save disk space.
    The `--rm=false` flag tells Docker to **keep** these intermediate containers after the build process.
    
    ### Why Keep Intermediate Containers?
    
    - **Debugging:** If your build fails, you can inspect the intermediate containers to understand the issue.
    - **Performance:** In some cases, keeping intermediate containers might improve build performance, especially when using Docker BuildKit. However, this behavior is not guaranteed and might depend on your build context and image complexity.
    
    **In summary,** the `--rm=false` flag is useful for debugging purposes or in specific scenarios where you need to inspect intermediate build stages. However, it's generally recommended to use the default behavior of removing intermediate containers to save disk space.
    

### **Docker Inspect Command**

- 
    
    The `docker inspect` command retrieves detailed information about Docker objects, such as containers, images, volumes, or networks. It provides a JSON output containing configuration and state details, which can be useful for debugging, monitoring, or verifying the state of a Docker object.
    
    ### Key Functions:
    
    - **Retrieves detailed information**: `docker inspect` can be used to gather comprehensive details about a Docker object, such as its configuration, runtime settings, network settings, and more.
    - **Useful for debugging**: Since the command provides a lot of detailed information, it's commonly used for debugging purposes to understand the state of a container, image, or other Docker objects.
    - **JSON Output**: The output is provided in JSON format, making it easy to parse and integrate with other tools or scripts.
    
    ### Command Structure:
    
    - **`docker inspect [OPTIONS] NAME|ID [NAME|ID...]`**:
        - **`NAME|ID`**: The name or ID of the Docker object to inspect (e.g., container name, image ID).
        - **`[OPTIONS]`**: Optional flags to customize the output.
    
    ### Common Options:
    
    - **`f` or `-format`**: Format the output using a Go template, allowing you to extract specific information.
    - **`-type`**: Specifies the type of object (`container`, `image`, `network`, `volume`), useful if multiple types share the same name or ID.
    
    ### Example:
    
    To inspect a running container named `web_app`, you would use:
    
    - **Command**: `docker inspect web_app`
    - **Example Output**:
    
    ```json
    jsonCopy code
    [
        {
            "Id": "e90e34656806",
            "Created": "2023-08-10T14:22:31.383Z",
            "Path": "nginx",
            "Args": [],
            ...
        }
    ]
    
    ```
    
    - **`docker inspect --format '{{ .NetworkSettings.IPAddress }}' web_app`**: This command retrieves only the IP address of the `web_app` container, using a Go template to format the output.
    

### **Understanding Volumes, Bind Mounts, and tmpfs in Docker**

- 
    
    ### Volumes
    
    - **Dedicated storage:** Docker creates a dedicated directory on the host machine to store data.
    - **Persistence:** Data persists even if the container is removed or stopped.
    - **Management:** Docker handles the lifecycle of the volume.
    - **Example:**
        
        `docker volume create my-data
        docker run -v my-data:/app my-image`
        
    
    ### Bind Mounts
    
    - **Host directory sharing:** Directly shares a directory from the host machine with the container.
    - **Persistence:** Data persists as it's on the host filesystem.
    - **Management:** You manage the host directory.
    - **Example:**
        
        `docker run -v /path/on/host:/path/in/container my-image`
        
    
    ### tmpfs
    
    - **In-memory storage:** Creates a temporary filesystem in the container's memory.
    - **No persistence:** Data is lost when the container stops.
    - **Performance:** Faster than volumes or bind mounts due to in-memory access.
    - **Example:**
        
        `docker run --mount type=tmpfs,destination=/tmp my-image`
        
    
    ### When to Use Which
    
    - **Volumes:** For data that needs to persist beyond the container's lifecycle.
    - **Bind mounts:** For sharing specific files or directories between the host and container, or when performance is critical.
    - **tmpfs:** For temporary data that doesn't need to persist, like cache or session data.
    
    **In summary:**
    
    - **Volumes** are ideal for persistent data.
    - **Bind mounts** are useful for sharing specific host directories with the container.
    - **tmpfs** is suitable for temporary data that needs fast access.
    

### **Docker Volume**

- 
    
    A Docker volume is a persistent data storage mechanism used to store data outside the container's file system. Volumes allow data to persist even after the container is removed and can be shared between multiple containers.
    
    ### Key Functions:
    
    - **Persistent Storage**: Volumes retain data even when containers are stopped or removed, providing a way to manage persistent data.
    - **Data Sharing**: Volumes can be shared between multiple containers, allowing them to read and write data to the same location.
    - **Decoupling Data**: By using volumes, you can separate data from the container's lifecycle, making it easier to manage and back up.
    
    ### Volume Locations:
    
    - On a Linux-based system, Docker volumes are typically stored in the `/var/lib/docker/volumes/` directory.
    - Each volume has its own subdirectory within this path, named after the volume's unique identifier.
    
    ### Command Structure:
    
    - **`docker volume create [OPTIONS] [VOLUME_NAME]`**: Creates a new volume with the specified name.
    - **`docker volume ls`**: Lists all Docker volumes on the host.
    - **`docker volume inspect [VOLUME_NAME]`**: Provides detailed information about a specific volume, including its mount point and usage.
    - **`docker volume rm [VOLUME_NAME]`**: Removes a specified volume.
    
    ### Example Commands:
    
    - **Creating a Volume**:
        - Command: `docker volume create my_volume`
        - This creates a new volume named `my_volume`.
    - **Listing Volumes**:
        - Command: `docker volume ls`
        - This shows all volumes on the host.
    - **Inspecting a Volume**:
        - Command: `docker volume inspect my_volume`
        - This provides detailed information about `my_volume`, including its location in `/var/lib/docker/volumes/`.
    
    ### Example Use Case:
    
    When running a container that needs to store data persistently:
    
    - **Command**: `docker run -v my_volume:/app/data my_image`
    - **Explanation**: This command runs a container using the image `my_image`, with the volume `my_volume` mounted to the `/app/data` directory inside the container. Data written to `/app/data` will be stored in `my_volume` and persist even if the container is removed.
    

### **Multi-Stage Builds in Docker**

- 
    
    **Multi-stage builds** are a feature in Docker that allows you to use multiple `FROM` statements in a single `Dockerfile`. This approach helps reduce the final image size by only copying the necessary artifacts from one stage to another, while discarding unnecessary build dependencies.
    
    ### Key Benefits:
    
    - **Smaller Image Size**: By copying only the final build artifacts to the production image, you avoid shipping unnecessary files, resulting in smaller and more efficient images.
    - **Simplified Builds**: Multi-stage builds make it easier to manage complex build processes, as you can separate different stages of the build process into different sections of the `Dockerfile`.
    
    ### How Multi-Stage Builds Work:
    
    1. **First Stage**:
        - Usually involves building your application.
        - This stage includes all the dependencies and tools needed to compile your code.
    2. **Final Stage**:
        - Only the necessary artifacts (like the compiled binaries) are copied from the previous stage.
        - This stage usually runs on a lightweight base image.
    
    ### Commands and Instructions Used:
    
    - **`FROM`**: Used to define each stage of the build. The first `FROM` starts the first stage, and each subsequent `FROM` starts a new stage.
    - **`AS`**: Used to name a stage, so you can reference it in later stages.
    - **`COPY --from=`**: Copies artifacts from a previous stage.
    
    ### Example Dockerfile:
    
    This example demonstrates a multi-stage build for a Go application:
    
    ```
    dockerfileCopy code
    # First stage: Build the application
    FROM golang:1.18-alpine AS build
    
    WORKDIR /app
    COPY . .
    
    # Compile the application
    RUN go build -o myapp
    
    # Second stage: Create the final image
    FROM alpine:latest
    
    WORKDIR /app
    
    # Copy the built application from the first stage
    COPY --from=build /app/myapp .
    
    # Command to run the application
    CMD ["./myapp"]
    
    ```
    
    ### Explanation:
    
    1. **First Stage (`build`)**:
        - **`FROM golang:1.18-alpine AS build`**: Uses a Go base image and names this stage `build`.
        - **`WORKDIR /app`**: Sets the working directory to `/app`.
        - **`COPY . .`**: Copies all files from the current directory into the container.
        - **`RUN go build -o myapp`**: Compiles the Go application and creates an executable named `myapp`.
    2. **Second Stage (Final Image)**:
        - **`FROM alpine:latest`**: Uses a minimal Alpine Linux image.
        - **`WORKDIR /app`**: Sets the working directory.
        - **`COPY --from=build /app/myapp .`**: Copies the compiled application from the `build` stage into the final image.
        - **`CMD ["./myapp"]`**: Specifies the command to run the application.
            
            
            **Example:**
            
            Imagine you're building a Go application. You need Go tools to build the application, but you don't need these tools in the final running image.
            
            - **Stage 1:** Use a Go image to build the application.
            - **Stage 2:** Use a minimal image (like Alpine) to copy the built application and run it.
            
            This way, your final image is much smaller and more efficient.
            
    

### **Docker Compose**

- 
    
    **Docker Compose** is a tool used to define and manage multi-container Docker applications. With Docker Compose, you can describe a set of services, networks, and volumes in a single YAML file, making it easier to deploy and manage applications that consist of multiple containers.
    
    **Imagine you're building a Lego castle.** You have many different pieces (containers) that need to fit together perfectly. **Docker Compose** is like a blueprint that tells you how to assemble these pieces.
    
    **Docker Compose** 
    1. **Orchestration tool:** It helps you define and run multi-container Docker applications.
    
    2. **YAML configuration:** You describe your application's services, networks, and volumes in a `docker-compose.yml` file.
    
    3. **Single command:** Start, stop, and rebuild your entire application with a single command: `docker-compose up`.
    
    ### Key Concepts:
    
    - **Service**: Represents a container or set of containers running the same image. For example, a web application and a database would be defined as separate services.
    - **Network**: Docker Compose allows you to define custom networks so that services can communicate with each other.
    - **Volume**: Allows you to persist data across container restarts and share data between services.
    
    ### Docker Compose Commands:
    
    - **`docker-compose up`**: Creates and starts all the containers defined in the `docker-compose.yml` file. It also creates any necessary networks and volumes.
    - **`docker-compose down`**: Stops and removes all containers, networks, and volumes created by `docker-compose up`.
    - **`docker-compose build`**: Builds the images specified in the `docker-compose.yml` file.
    - **`docker-compose logs`**: Displays logs from the running containers.
    
    ### Docker Compose YAML File Structure:
    
    The `docker-compose.yml` file defines the services, networks, and volumes for your application. Below are common directives used in a `docker-compose.yml` file:
    
    - **`version`**: Specifies the version of the Docker Compose file format.
    - **`services`**: Defines the individual services (containers) that make up your application.
    - **`image`**: Specifies the image to use for the service.
    - **`build`**: Specifies the context (directory) and other build options for creating a custom image.
    - **`ports`**: Maps container ports to host ports.
    - **`volumes`**: Mounts host paths or named volumes to the container.
    - **`environment`**: Defines environment variables for the container.
    - **`networks`**: Specifies custom networks for the services to communicate with each other.
    
    ### Example Docker Compose File:
    
    Here’s an example of a `docker-compose.yml` file for a simple web application with a database:
    
    ```yaml
    version: '3.8'
    
    services:
      web:
        image: nginx:latest
        ports:
          - "8080:80"
        volumes:
          - ./html:/usr/share/nginx/html
        networks:
          - app-network
    
      database:
        image: mysql:5.7
        environment:
          MYSQL_ROOT_PASSWORD: example
          MYSQL_DATABASE: exampledb
        volumes:
          - db_data:/var/lib/mysql
        networks:
          - app-network
    
    volumes:
      db_data:
    
    networks:
      app-network:
    
    ```
    
    ### Explanation of the Example:
    
    - **Version**: Specifies that we are using version 3.8 of the Docker Compose file format.
    - **Services**:
        - **`web`**:
            - **`image: nginx:latest`**: Uses the latest Nginx image.
            - **`ports: "8080:80"`**: Maps port 80 of the container to port 8080 on the host.
            - **`volumes: ./html:/usr/share/nginx/html`**: Mounts the `./html` directory from the host to the container’s Nginx HTML directory.
            - **`networks: app-network`**: Connects the web service to the `app-network`.
        - **`database`**:
            - **`image: mysql:5.7`**: Uses the MySQL 5.7 image.
            - **`environment`**: Sets environment variables like `MYSQL_ROOT_PASSWORD` and `MYSQL_DATABASE`.
            - **`volumes: db_data:/var/lib/mysql`**: Uses a named volume `db_data` to persist the database files.
            - **`networks: app-network`**: Connects the database service to the `app-network`.
    - **Volumes**:
        - **`db_data`**: Defines a named volume that stores the database data.
    - **Networks**:
        - **`app-network`**: Defines a custom network that allows the `web` and `database` services to communicate with each other.
    

> Public Registries:
Dockerhub
> 
> 
> AWS --> ECR --> Elastic Container Registry Google --> 
> GCR --> Google Container Registry
> AZURE --> ACR --> Azure Container Registry
> 
> Version Controls / Source controls / Code Repositories:
> 
> GitHub 
> Gitlab
> HGmercurial
> SVN
> 

## Docker in Cloud

### Pushing and Pulling Docker Images to ECR with IAM Login

- 
    
    ### Understanding the Process
    
    To push and pull Docker images to and from Amazon Elastic Container Registry (ECR), you'll need:
    
    1. **An AWS account:** With necessary permissions to access ECR.
    2. **Docker installed:** On your local machine.
    3. **AWS CLI configured:** With appropriate credentials.
    4. **An ECR repository:** Created in your AWS account.
    
    ### Steps Involved
    
    ### 1. Create an IAM User with ECR Permissions
    
    - Create an IAM user with programmatic access.
    - Attach the necessary IAM policies for ECR actions (e.g., `AmazonEC2ContainerRegistryFullAccess`).
    - Generate access keys for the user.
    # The instructor also added the keys to env 
    `export AWS_ACCESS_KEY_ID="paste here"
    export AWS_SECRET_ACCESS_KEY="Paste Here"`
    
    ### 2. Configure AWS CLI
    
    - Install the AWS CLI.
    - Configure the AWS CLI with your access keys and region.  [1. Set up the AWS CLI - AWS Command Line Interface - AWS Documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html)
        
        [docs.aws.amazon.com](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html)
        
        > awscli installation
        > 
        > 
        > **`curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        > unzip awscliv2.zip
        > sudo ./aws/install --bin-dir** /usr/local/bin **--install-dir** /usr/local/aws-cli **--update**`
        > 
        
    
    ### 3. Authenticate Docker to ECR
    
    - Generate a temporary authentication token:
    
    Replace `<your-region>` with your AWS region and `<account-id>` with your AWS account ID.
        
        `aws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <account-id>.dkr.ecr.<region>.amazonaws.com` 
        
    
    ### 4. Push an Image to ECR
    
    - Tag your local image with the ECR repository URI:
        
        Bash
        
        `docker tag <image-name> <account-id>.dkr.ecr.<region>.amazonaws.com/<repository-name>:<tag>`
        
    
    - Push the image:
        
        Bash
        
        `docker push <account-id>.dkr.ecr.<region>.amazonaws.com/<repository-name>:<tag>`
        
    
    ### 5. Pull an Image from ECR
    
    - Authenticate to ECR (if not already done).  [1. Moving an image through its lifecycle in Amazon ECR  
    docs.aws.amazon.com](https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html#:~:text=typing%20Ctrl%20%2B%20c.-,Step%202%3A%20Authenticate%20to%20your%20default%20registry,you%20want%20to%20authenticate%20to.)
    - Pull the image:
        
        Bash
        
        `docker pull <account-id>.dkr.ecr.<region>.amazonaws.com/<repository-name>:<tag>`
        
    
    ### Additional Considerations
    
    - **IAM Roles:** For more complex scenarios, consider using IAM roles for authentication instead of access keys.
    - **ECR Image Scanning:** ECR offers built-in image scanning for vulnerabilities.
    - **ECR Lifecycle Policy:** Manage image retention and cleanup policies.
    - **Docker CLI Configuration:** You can configure Docker to use ECR as a default registry for simplified commands.

### **Amazon Elastic Container Service (ECS)**

- 
    
    ### **Amazon Elastic Container Service (ECS)**
    
    **Overview**
    
    - **Amazon Elastic Container Service (ECS)** is a fully managed container orchestration service provided by AWS (Amazon Web Services). It allows you to run and manage Docker containers (small, isolated environments for running applications) on a cluster of EC2 instances (virtual machines) or using Fargate (a serverless compute engine for containers, meaning you don't have to manage the underlying servers).
    
    ---
    
    ### **Key Features of ECS**
    
    **Container Orchestration**
    
    - **Deployment**: ECS handles setting up containers.
    - **Scaling**: ECS adjusts the size of your container resources.
    - **Management**: ECS manages containers across a cluster of EC2 instances or Fargate.
    - **Scheduling**: Containers are scheduled based on resource requirements (e.g., CPU and memory), availability, and custom-defined rules (specific instructions).
    
    **Task Definitions**
    
    - **What it is**: ECS uses task definitions, which are JSON files (text files in a specific format), to define how Docker containers should run.
    - **Contents**: Task definitions include details like which Docker image to use, CPU/memory allocation, environment variables (settings), and networking configurations (how it connects to other services).
    
    **Clusters**
    
    - **Definition**: A cluster is a logical grouping (organization) of EC2 instances or Fargate tasks where your containers run.
    - **Lifecycle Management**: ECS manages the cluster's lifecycle (all stages from creation to deletion) and scales up or down as needed.
    
    **Service Management**
    
    - **Deployment and Scaling**: ECS can automatically manage the deployment and scaling of your containerized applications by defining services (groups of tasks running together).
    - **Reliability**: ECS services ensure that the specified number of tasks (containers) are always running.
    - **Features**: ECS handles load balancing (distributing traffic), auto-scaling (automatically adjusting resources), and rolling updates (gradually updating the application without downtime).
    
    **Integration with AWS Services**
    
    - **Elastic Load Balancing (ELB)**: For evenly distributing incoming requests.
    - **AWS Identity and Access Management (IAM)**: For managing security.
    - **Amazon CloudWatch**: For monitoring performance and health of your applications.
    
    **ECS vs. EKS**
    
    - **ECS**: An AWS-native container management service, meaning it’s specifically built for AWS.
    - **EKS (Elastic Kubernetes Service)**: A managed Kubernetes service for those who prefer Kubernetes (another container orchestration tool) for managing containers.
    
    **Fargate**
    
    - **Definition**: Fargate is an option within ECS that lets you run containers without managing the underlying EC2 instances (you don't need to worry about setting up or maintaining virtual servers).
    - **Simplification**: With Fargate, AWS handles the server infrastructure, allowing you to focus on your containers and applications without managing the servers they run on. It’s like using a service where the computing power is provided without needing to set up or manage the actual computers.
    
    ---
    
    ### **How ECS Works**
    
    1. **Create a Task Definition**:
        - Define your container specifications, such as Docker image (the application you want to run), CPU/memory requirements (processing power and memory needed), and environment variables (app settings).
    2. **Create a Cluster**:
        - Set up a cluster (a group of EC2 instances or Fargate tasks) to run your containers.
        - **Options**: Use either a cluster of EC2 instances (virtual servers) or AWS Fargate for a serverless option (no server management required).
    3. **Deploy a Service or Task**:
        - Deploy your application by creating a service (a group of tasks running together) or running a task (a single instance of your container).
        - **ECS Management**: ECS ensures your containers are running as defined.
    4. **Monitor and Scale**:
        - Use ECS features like auto-scaling (automatically adjusting resources) and monitoring (tracking performance) to ensure your application can handle changes in load (increasing or decreasing demand).
    
    ---
    
    ### **Example Workflow**
    
    1. **Define a Task Definition**:
        - Set up a task definition that specifies the Docker image (the app) and container settings.
    2. **Create a Cluster**:
        - Set up an ECS cluster on EC2 or Fargate.
    3. **Deploy a Service**:
        - Deploy a service based on your task definition.
    4. **Monitor with CloudWatch**:
        - Set up CloudWatch alarms (alerts) to monitor the health and performance of your ECS services.
    
    ---
    
    ### **Summary**
    
    - **ECS** simplifies running containerized applications in the cloud by providing scalability (easy to expand), reliability (stable and dependable), and integration with other AWS services.
    - **Fargate**: As part of ECS, Fargate further simplifies things by removing the need to manage the underlying server infrastructure, allowing you to focus purely on your containers and applications.

> **Github**
> 
> 
> How to generate Token PAT --> Profile --> Settings --> Developer Setting --> PAT
> 
> GIT_PAT=<token>
> 
> echo $GIT_PAT | docker login ghcr.io -u abdealidevops --password-stdin
> 
> docker tag <old image> <githublocation/username/imagename>
> 
> docker tag busybox ghcr.io/abdealidevops/mybusybox:v1
> 
> docker push ghcr.io/abdealidevops/mybusybox:v1
> 

> Also use Gitlab 
create a new project and then start the gitlab’s web IDE
> 
> 
> **GitLab CI/CD Pipeline: A Simplified Explanation**
> 
> A GitLab CI/CD pipeline is an automated workflow that builds, tests, and deploys your code.
> 
> Imagine a factory assembly line. Each step in the pipeline is like a station on the assembly line. The pipeline takes your code (raw materials), processes it through various stages (building, testing, deploying), and produces a finished product (a deployed application).
> 
> Gitlab--> Menu --> Your project --> CI/CD --> Pipeline --> Running or passed or failed --> DockerBUILD --> Process running in background
> 
> How to show your image file from GITLAB workflow
> 
> Gitlab--> Menu --> Your project --> package & Registries --> container_registry-->
> 
> Gitlab to ECR
> 
> Go to Gitlab account and create variable
> 
> Your project --> Project click --> Setting --> CICD --> Variables --> Expand
> 
> AWS_ACCESS_KEY_ID  -->
> 
> AWS_SECRET_ACCESS_KEY -->
> 
> AWS_DEFAULT_REGION -->>
> 
> You need to define value as per your AWS token ID and Region --> Then you need to go AWS ECR account and copy the location
> 
> ECR --> <link to your ECR repo>
> 
> in the .yml You need to change
> 
> DOCKER_REGISTRY:
> 
> AWS_DEFAULT_REGION
> 
> APP_NAME
> 
> Gitlab push image
> 
> Gitlab push to ECR
> 

## Docker Networking Overview

- 
    
    Docker networking allows containers to communicate with each other and the outside world. By default, Docker sets up several networks that containers can be attached to, and you can create custom networks to fit specific needs.
    
    ### Key Docker Network Concepts
    
    - **Container Network**: A logical connection between containers, allowing them to communicate with each other.
    - **Bridge Network**: The default network for Docker containers. Containers on the same bridge network can communicate with each other.
    - **Host Network**: Allows a container to share the host's network stack. There is no network isolation between the container and the host.
    - **Overlay Network**: Used for multi-host Docker setups. It enables containers running on different Docker hosts to communicate securely.
    - **None Network**: Disables networking for the container. It can be used when the container doesn’t need any network communication.
    - **Network Driver**: A plugin that manages how networking is implemented for a container. Examples include `bridge`, `host`, `overlay`, and custom drivers.
    
    ### Default Docker Networks
    
    1. **Bridge Network**:
        - This is the default network that containers connect to if no other network is specified.
        - Containers can communicate with each other using IP addresses or container names.
    2. **Host Network**:
        - Containers share the network namespace of the host.
        - There is no network isolation between the container and the host machine.
    3. **None Network**:
        - Disables networking completely for a container.
    4. **Overlay Network**:
        - Used in Docker Swarm or multi-host networking.
        - Enables communication between containers on different Docker hosts.
    
    ### Common Docker Networking Commands
    
    - **List Networks**:
        - `docker network ls`: Lists all available Docker networks.
    - **Inspect a Network**:
        - `docker network inspect <network-name>`: Provides detailed information about a network, such as connected containers and configurations.
    - **Create a Custom Network**:
        - `docker network create --driver <driver-name> <network-name>`: Creates a new network with the specified driver (e.g., bridge, overlay).
    - **Connect a Container to a Network**:
        - `docker network connect <network-name> <container-name>`: Connects an existing container to a specific network.
    - **Disconnect a Container from a Network**:
        - `docker network disconnect <network-name> <container-name>`: Disconnects a container from a network.
    
    ### Example: Creating and Using a Custom Bridge Network
    
    1. **Create a Custom Bridge Network**:
        - `docker network create my-bridge-network`
    2. **Run a Container in the Custom Network**:
        - `docker run -d --name my-container --network my-bridge-network nginx`
    3. **Inspect the Network**:
        - `docker network inspect my-bridge-network`
    
    ### Custom Network Use Case
    
    Custom networks are useful when you want to isolate containers or control how they communicate with each other. For instance, you might want to set up a backend and frontend network where the backend network is only accessible to the frontend containers.
    
    ### Networking with Docker Compose
    
    When using Docker Compose, you can define networks in the `docker-compose.yml` file and specify which containers connect to which networks. This allows for more complex setups with multiple services.
    
    Example `docker-compose.yml`:
    
    ```yaml
    version: '3'
    services:
      web:
        image: nginx
        networks:
          - frontend
      db:
        image: mysql
        networks:
          - backend
    networks:
      frontend:
      backend:
      
    ```
    
    In this example, the `web` service connects to the `frontend` network, while the `db` service connects to the `backend` network, providing isolation between the frontend and backend services.
    
    ### Conclusion
    
    Docker networking is a flexible system that allows containers to communicate with each other and external systems. By understanding and using different network types and commands, you can tailor your container's networking to suit your specific application requirements.
    

> Watch these for better understanding:
1. [https://www.youtube.com/watch?v=zJD7QYQtiKc](https://www.youtube.com/watch?v=zJD7QYQtiKc)

2. [https://www.youtube.com/watch?v=bKFMS5C4CG0](https://www.youtube.com/watch?v=bKFMS5C4CG0)
> 

### IPAM and Network Drivers in Docker

- 
    
    ### IPAM (IP Address Management)
    
    IPAM is responsible for allocating IP addresses to containers within a network. It ensures that IP addresses are assigned uniquely and efficiently. Docker includes a built-in IPAM driver, but you can also use third-party IPAM drivers for more complex networking scenarios.
    
    **Key responsibilities of IPAM:**
    
    - Assigning IP addresses to containers
    - Managing address ranges and subnets  [](https://docs.docker.com/compose/compose-file/06-networks/#:~:text=outside%3A%20external%3A%20true-,ipam,mapping%20from%20hostname%20to%20IP)
    - Handling IP address allocation and deallocation
    
    ### Network Drivers
    
    Network drivers define the underlying networking technology used by Docker. They determine how containers communicate with each other and the outside world. Docker provides several built-in network drivers, and you can also use third-party drivers.
    
    **Common network drivers:**
    
    - **bridge:** Creates a software bridge for communication between containers.  [](https://docs.docker.com/network/drivers/bridge/#:~:text=In%20terms%20of%20Docker%2C%20a,network%20communicate%2C%20while%20providing%20isolation)
    - **host:** Shares the host's network stack with the container.
    - **overlay:** Creates an overlay network for communication between containers across multiple hosts.  [](https://docs.docker.com/network/drivers/overlay/#:~:text=The%20overlay%20network%20driver%20creates,among%20multiple%20Docker%20daemon%20hosts.)
    - **macvlan:** Creates virtual network interfaces for containers.  [](https://docs.docker.com/network/drivers/macvlan/#:~:text=In%20this%20type%20of%20situation,making%20it%20appear%20to%20be)
    
    ### Relationship between IPAM and Network Drivers
    
    - **IPAM driver:** Manages IP address allocation within a network.
    - **Network driver:** Defines the underlying network topology and connectivity.
    
    **Example:**
    
    When you create a Docker network using the `bridge` driver, Docker's built-in IPAM driver will allocate IP addresses from a default address range to containers connected to that network.
    

### Container Network Model (CNM)

- 
    
    The Container Network Model (CNM) is a specification that defines how containers should communicate with each other and the outside world.
    
    It provides a standardized way for container runtimes and orchestration systems to manage network connectivity.
    
    ### Key Components of CNM
    
    - **Sandbox:** Represents an isolated network environment for a container. It includes IP address, MAC address, routes, and DNS settings.
    - **Endpoint:** Represents a network interface within a sandbox. A container can have multiple endpoints, each connected to a different network.
    - **Network:** A group of endpoints that can communicate with each other directly.

### Docker network commands

- 
    
    Create a new network:
    
    `docker network create <network-name>`
    
    List all networks:
    
    `docker network ls`
    
    Inspect a network's details:
    
    `docker network inspect <network-name>`
    
    Connect a container to a network:
    
    `docker network connect <network-name> <container-name>`
    
    Disconnect a container from a network:
    
    `docker network disconnect <network-name> <container-name>`
    
    Remove a network:
    
    `docker network rm <network-name>`
    
    Run a container on a specific network:
    
    `docker run --network <network-name> <image-name>`
    

### **Docker linking**

- 
    
    is an older method of connecting Docker containers, allowing them to communicate with each other. However, it has largely been replaced by Docker networks, which provide more flexibility and features.
    
    ### Key Docker Linking Commands:
    
    Link a container to another container:
    
    `docker run --link <container-name-or-id>:<alias> <image-name>`
    
    Example:
    
    If you have a container named `db` running a database and want to link it to a new container named `web`, you could use:
    
    `docker run --link db:db_alias <image-name>`
    
    ### How Linking Works:
    
    - **`-link <container-name-or-id>:<alias>`**: This option creates a connection between containers. The `alias` is an optional name you can give to the link, which can be used inside the new container to reference the linked container.
    
    ### Important Notes:
    
    - Linked containers can access each other's environment variables and exposed ports.
    - **Deprecation**: Docker linking is now considered legacy, and the preferred method is using Docker networks.

### Docker Swarm

- 
    
    Docker Swarm is Docker's native clustering and orchestration tool, allowing you to create and manage a cluster of Docker nodes (machines). It transforms a group of Docker engines into a single virtual Docker engine. Docker Swarm is used for deploying, scaling, and managing containerized applications in a distributed environment.
    
    ### Key Concepts
    
    - **Swarm Mode**: A mode that turns a Docker engine into a manager or worker in a swarm cluster. This is where all the orchestration happens.
    - **Manager Node**: The control node in a swarm that manages the entire cluster, distributing tasks to worker nodes, handling scaling, and maintaining the desired state.
    - **Worker Node**: A node that receives and executes tasks from the manager node. It does not manage or orchestrate.
    - **Service**: A definition of a task that should be run on a swarm. Services define the containers to run and how they should be replicated.
    - **Task**: The smallest unit of work in Docker Swarm, representing a single container running as part of a service.
    
    ### Key Docker Swarm Commands
    
    Initialize a Swarm:
    
    `docker swarm init`
    
    Join a node to an existing Swarm:
    
    `docker swarm join --token <token> <manager-ip>:<port>`
    
    List all nodes in the Swarm:
    
    `docker node ls`
    
    Create a new service in the Swarm:
    
    `docker service create --name <service-name> <image-name>`
    
    List all services in the Swarm:
    
    `docker service ls`
    
    Scale a service to a specified number of replicas:
    
    `docker service scale <service-name>=<replica-count>`
    
    Remove a service from the Swarm:
    
    `docker service rm <service-name>`
    
    Leave the Swarm (for a node):
    
    `docker swarm leave`
    
    ### Benefits of Docker Swarm
    
    - **Scalability**: Easily scale your applications up or down with simple commands.
    - **High Availability**: Manager nodes in a Swarm can be configured to failover automatically in case of failure.
    - **Load Balancing**: Swarm automatically distributes tasks across worker nodes, ensuring optimal use of resources.
    - **Declarative Service Model**: You can declare the desired state of services, and Swarm manages the actual state to match the desired state.
    
    ### Important Notes
    
    - Docker Swarm is simpler to use than Kubernetes but offers fewer advanced features.
    - It's tightly integrated with Docker, making it a good choice for those already familiar with Docker and looking for a straightforward orchestration tool.

### **Docker Secrets**

- 
    
    is a feature that allows you to securely manage sensitive information such as passwords, SSH keys, or API tokens in a Docker Swarm environment. Instead of embedding sensitive data directly in your application code or passing it through environment variables, Docker Secrets enables you to store and manage this data securely.
    
    ### Key Concepts
    
    - **Secure Storage:** Docker secrets are encrypted at rest and in transit.
    - **Docker Swarm Requirement:** Docker Secrets is a feature of Docker Swarm, so it’s primarily used in Swarm mode.
    - **Secure by Design:** Secrets are never written to disk in plaintext and are automatically removed when no longer in use, however they are stored in memory when the contaner is active can be be seen unencrypted in `/run/secret/`.
    - **Scoped Access:** Secrets are available only to the containers that need them, and they are never exposed outside of the service that consumes them.
    
    ### Commands Related to Docker Secrets
    
    Create a new secret:
    
    `docker secret create <secret-name> <secret-file>`
    
    List all secrets:
    
    `docker secret ls`
    
    Inspect a specific secret (to view its metadata, not the secret's content):
    
    `docker secret inspect <secret-name>`
    
    Remove a secret:
    
    `docker secret rm <secret-name>`
    
    Use a secret in a service:
    
    `docker service create --name <service-name> --secret <secret-name> <image>`
    
    ### Example Workflow
    
    1. **Create a Secret:**
        
        You have a password file named `db_password.txt` that you want to use as a secret.
        
        - Command: `docker secret create db_password db_password.txt`
    2. **Deploy a Service with the Secret:**
        
        You want to deploy a MySQL container that uses this secret.
        
        - Command: `docker service create --name mysql --secret db_password mysql:latest`
    3. **Access the Secret in a Container:**
        
        Inside the container, the secret will be available at `/run/secrets/db_password`.
        
    

### **Local Docker Repository**

A local Docker repository is a location on your machine where you can store Docker images. This is useful when you want to reuse images without pulling them from a remote registry like Docker Hub every time or when you're working in an environment without internet access.

### Key Concepts

- **Local Registry**: A Docker registry that is hosted on your local machine or within your private network. It allows you to push and pull images without using a public service like Docker Hub.
- **Image Caching**: When you build or pull Docker images, they are stored locally in the Docker cache. This allows you to run containers from these images without needing to download them again.

> Default port for local registry is 5000
> 

### Setting Up a Local Docker Registry

You can set up a local Docker registry using the official Docker Registry image.

### Steps to Set Up a Local Docker Registry

Pull the official Docker Registry image:

`docker pull registry`

Run a container with the registry image:

`docker run -d -p 5000:5000 --name my-local-registry registry`

Push an image to your local registry:

`docker tag <image-name> localhost:5000/<image-name>`

`docker push localhost:5000/<image-name>`

Pull an image from your local registry:

`docker pull localhost:5000/<image-name>`

### Using a Local Repository for Development

- **Faster Builds**: Using a local registry can speed up your development process by reducing the time needed to pull images from a remote source.
- **Testing**: You can test images locally before pushing them to a production registry.
- **Offline Work**: A local registry allows you to work in environments without internet access.

### List Images in Your Local Registry

To see the images in your local Docker registry, you need to interact with the registry API. You can do this with a simple curl command if your local registry is running on `localhost:5000`:

List repositories (which include images) in the local registry:

`curl http://localhost:5000/v2/_catalog`

List tags of a specific repository (replace `<repository-name>` with the actual name):

`curl http://localhost:5000/v2/<repository-name>/tags/list`

This will return a list of repositories and their associated tags (versions) stored in your local registry.

### 3. Use a Browser (Optional)

You can also view the local Docker registry's contents by navigating to `http://localhost:5000/v2/_catalog` in your web browser. It will show you the list of repositories (images) stored in the local registry.

### **Docker Export, Import, Load, and Save**

These Docker commands are used to manage images and containers, especially for moving them between environments or creating backups.

### Export a Container

Command:

`docker export <container-id> -o <filename.tar>`

- **Description:** Exports the file system of a running container into a tarball (tar file). This is useful for creating a snapshot of a container's state.

### Import an Exported Container

Command:

`docker import <filename.tar>`

- **Description:** Creates an image from an exported container's tarball. This can be used to recreate a container as an image.

### Save an Image

Command:

`docker save -o <filename.tar> <image-name>`

- **Description:** Saves one or more Docker images to a tarball. This allows you to back up images or move them between systems.

### Load an Image

Command:

`docker load -i <filename.tar>`

- **Description:** Loads an image from a tarball that was created using the `docker save` command. This can be used to restore images on a different machine.

### Example Workflow

1. **Export a Running Container:**
    - Command: `docker export my-container -o my-container.tar`
    - This creates a tarball of the container's file system.
2. **Import the Exported Container:**
    - Command: `docker import my-container.tar`
    - This converts the tarball back into an image.
3. **Save an Image:**
    - Command: `docker save -o my-image.tar my-image:latest`
    - This saves the image `my-image` to a tarball.
4. **Load an Image:**
    - Command: `docker load -i my-image.tar`
    - This loads the image from the tarball back into Docker.

- Docker Monitoring:
you can use docker stats or 3rd party tools like ctop

# Podman

### **Podman Overview**

**What is Podman?**

Podman (short for *Pod Manager*) is a **daemonless**, **open-source** container engine for developing, managing, and running containers and pods on Linux systems.

Think of it like Docker, but with more security and flexibility in certain areas.

---

### 🚀 **Key Features**

### ✅ **Daemonless Architecture**

- **No background service** is running.
- Each command is executed as a separate process.
- Makes Podman more secure and easier to audit (compared to Docker’s always-running daemon).

### ✅ **Rootless Containers**

- You can run containers as a **non-root user**, improving security.
- No need for `sudo` in most cases.

### ✅ **Docker-Compatible CLI**

- Podman’s commands are **almost identical** to Docker.
- You can often just replace `docker` with `podman` in commands.
    - Example: `docker run` → `podman run`

### ✅ **Pods Support (like Kubernetes)**

- A **pod** is a group of containers that share networking and namespaces.
- Just like Kubernetes, Podman supports pods natively.

### ✅ **Image and Container Management**

- Build, pull, push, and run containers using standard OCI-compliant images.
- Compatible with Docker images (can use images from Docker Hub or your own registries).

### ✅ **No Daemon, No Sockets (Unless You Want)**

- Podman can run as the current user and doesn't require a service or socket to interact with containers.

---

### 🛠️ **Podman vs Docker**

| Feature | Podman | Docker |
| --- | --- | --- |
| Daemonless | ✅ Yes | ❌ No |
| Rootless containers | ✅ Yes (built-in) | ⚠️ Partial (needs extra setup) |
| Docker-compatible CLI | ✅ Yes | ✅ Yes |
| Pod support | ✅ Yes (native) | ❌ No |
| Background service | ❌ No (on-demand execution) | ✅ Yes (Docker daemon) |

---

### 🧾 **Common Use Cases**

- Secure container deployments without root.
- Kubernetes-style pod creation on local machines.
- Lightweight container development environments.
- Compatibility testing for Kubernetes deployments.

### **Container Management with Podman**

### 🔹 Run a Container

Run a container from an image

`podman run [options] <image>`

Common flags:

- `it` → interactive + terminal (used with shells)
- `-rm` → auto-remove after exit
- `d` → run in background (detached)
- `-name` → assign a name to the container
- `p` → publish ports (`p 8080:80`)

Example:

`podman run -it --rm --name test-container -p 8080:80 alpine sh`

---

### 🔹 Start/Stop/Restart a Container

- `podman start <container>`
- `podman stop <container>`
- `podman restart <container>`

---

### 🔹 List Containers

- `podman ps` → list running containers
- `podman ps -a` → list **all** containers (including stopped)

---

### 🔹 Remove Containers

- `podman rm <container>` → remove container
- `podman rm -a` → remove **all** containers
- `podman rm -f <container>` → force remove

---

### 📦 **Image Management**

### 🔹 Pull Image

- `podman pull <image>`
    
    Example: `podman pull nginx`
    

### 🔹 List Images

- `podman images`

### 🔹 Remove Image

- `podman rmi <image>`
- `podman rmi -a` → remove **all** images

### 🔹 Build Image (from Dockerfile)

- `podman build -t <image-name> .`
    
    Example: `podman build -t myapp .`
    

---

### 🧰 **Inspect, Logs & Exec**

### 🔹 Inspect Container/Image

- `podman inspect <container|image>`

### 🔹 Logs

- `podman logs <container>`
- `podman logs -f <container>` → follow live logs

### 🔹 Exec Inside Running Container

- `podman exec -it <container> <command>`
    
    Example: `podman exec -it mycontainer bash`
    

---

### 🌐 **Networking**

### 🔹 List Networks

- `podman network ls`

### 🔹 Create Network

- `podman network create <name>`

### 🔹 Connect Container to Network

- `podman network connect <network> <container>`

### 🔹 Inspect Network

- `podman network inspect <network>`

---

### 🧪 **Pods in Podman**

### 🔹 Create Pod

- `podman pod create --name <pod-name>`

### 🔹 List Pods

- `podman pod ps`

### 🔹 Run Container Inside Pod

- `podman run --pod <pod-name> <image>`

### 🔹 Inspect Pod

- `podman pod inspect <pod-name>`

---

### 🔐 **Rootless vs Rooted**

Use `podman` directly (no `sudo`) for rootless.

Use `sudo podman` if you want to run containers as root (not recommended unless needed).

---

### 🔄 **Export/Import & Save/Load**

- `podman save -o <filename>.tar <image>` → save image to file
- `podman load -i <filename>.tar` → load image from file
- `podman export -o <file>.tar <container>` → export container filesystem
- `podman import <file>.tar` → import back as image

### Concept: What are CMD and ENTRYPOINT?

When you build a container image using a Dockerfile (Podman uses Dockerfiles too), you can define what command gets run **when the container starts**:

- **`CMD`**: Default command *if no other command is provided at run time*.
- **`ENTRYPOINT`**: Always executed when the container starts, *cannot be overridden unless forced*.

---

## 🧾 Syntax Forms

There are two ways to write both `CMD` and `ENTRYPOINT`:

### 1. **Exec Form** (Preferred)

- Format: `["executable", "param1", "param2"]`
- Doesn't invoke a shell (`/bin/sh`), so signals (like `CTRL+C`) are passed properly.
- Safer and more predictable.

Example:

`ENTRYPOINT ["nginx", "-g", "daemon off;"]`

`CMD ["--port", "8080"]`

---

### 2. **Shell Form**

- Format: `"executable param1 param2"` (as one string)
- Runs inside a shell: `/bin/sh -c`
- Useful if you want shell features like variable expansion, chaining (`&&`, `||`), etc.

Example:

`CMD "nginx -g 'daemon off;'"`

---

## 🔗 CMD vs ENTRYPOINT

| Feature | CMD | ENTRYPOINT |
| --- | --- | --- |
| Purpose | Default command | Main executable always run |
| Overridable | Yes (via `podman run <image> <cmd>`) | No (unless using `--entrypoint`) |
| Combined Use | CMD supplies default args to ENTRYPOINT | Yes |
| Common Use | Overrideable script/command | Fixed startup behavior (e.g., nginx) |

---

## 📦 How They Work Together

If both are defined in a Dockerfile:

```
ENTRYPOINT ["python3"]
CMD ["app.py"]
```

Running the container will execute:

`python3 app.py`

But you can override CMD like this:

`podman run myimage script.py` → becomes `python3 script.py`

To override ENTRYPOINT:

`podman run --entrypoint node myimage app.js` → replaces ENTRYPOINT with `node`

---

## 🚫 If You Use Only CMD

If you don’t specify `ENTRYPOINT`, `CMD` becomes the command.

```
CMD ["nginx", "-g", "daemon off;"]
```

---

## 🧪 Real Examples

### Example 1: ENTRYPOINT as fixed app, CMD as config

```
FROM ubuntu
ENTRYPOINT ["echo"]
CMD ["Hello from Podman"]
```

Result when container runs:

`echo Hello from Podman`

Override CMD:

`podman run myimage "Hi"` → `echo Hi`

---

### Example 2: Shell form (not ideal)

```
CMD echo Hello World
```

Runs using `/bin/sh -c 'echo Hello World'`

Not good for production where signal handling is needed.

---

### Summary

| You want to... | Use |
| --- | --- |
| Run a command that can be overridden | CMD |
| Set a command that should always run | ENTRYPOINT |
| Combine fixed entry with flexible arguments | ENTRYPOINT + CMD |
| Use signal-safe execution (better control) | Exec form |
| Use shell features like `&&`, `$VAR`, `*` | Shell form |